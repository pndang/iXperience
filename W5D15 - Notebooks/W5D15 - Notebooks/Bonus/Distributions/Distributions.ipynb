{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Distributions </font>\n",
    "\n",
    "As a Data Scientist you are probably not going to be using these distributions directly too often, but they form the basis of most of the techniques that you will be using regularly, so it’s useful to know a bit about them.\n",
    "\n",
    "There are a myriad different distributions, but we’ll just be focusing on a few of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(6,6)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of a variable is a description of the relative numbers of times each possible outcome will occur in a number of trials.\n",
    "\n",
    "The distribution of a statistical data set (or a population) is a listing or function showing all the possible values (or intervals) of the data and how often they occur.\n",
    "\n",
    "A probability distribution is a mathematical function that describes the values (and the respective probabilities) that a variable can assume. Think of them like \"models\" that explain what a variable looks like. And by variable we mean recordings of a specific phenomenon in the real world.\n",
    "\n",
    "Scipy has the most common distributions in the `scipy.stats` submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> 1. Normal Distribution </font>\n",
    "\n",
    "The Normal distribution (also called *Gaussian Distribution*, or *Bell Curve*) is a continous probability distribution, and its the most common distribution you'll find (that's why is normal!). \n",
    "\n",
    "It describes well a lot of random phenomenons, such as human height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation is:\n",
    "\n",
    "\n",
    "### $$ {\\mathcal {N}}(\\mu ,\\sigma ^{2}) $$\n",
    "\n",
    "\n",
    "Where the parameters are the mean (${\\mathcal \\mu \\in \\mathbb{R} } $), and the variance ($ { \\sigma ^{2}>0} $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Example </font>\n",
    "We can use the Normal distribution to represent Portuguese Men's height.\n",
    "\n",
    "We just need the parameters (taken from [this site](https://tall.life/height-percentile-calculator-age-country/)), which are:\n",
    "\n",
    "> $\\mathcal \\mu$ = 173.9 cm\n",
    "\n",
    "> $\\sigma ^{2}$ = $ 7.42 ^ {2}$ cm\n",
    "\n",
    "\n",
    "So our normal distribution is:\n",
    "## $ {\\mathcal {N}}(173.9, 55.0564) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some 10000 datapoints from this distribution.\n",
    "Since we are using a normal distribution, we'll use `stats.norm`. \n",
    "\n",
    "We'll use its `.rvs` method to generated some data from this distribution (a random sample, or a *random variate*). You can see that here, `loc` is the mean, and `scale` is the standard deviation (which gets squared by the function). (location and scale are defined for multiple distributions, [just happens that they are the mean and standard deviation for the normal distribution](https://www.itl.nist.gov/div898/handbook/eda/section3/eda364.htm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = stats.norm.rvs(size=10000, loc=173.9, scale=7.42, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of our simulate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the distictive bell-shape. The height of most men sits around in the center, with the extreme values being less frecuent.\n",
    "\n",
    "Now that we have the distribution, we can use some functions to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Cumulative Distribution Function (CDF)</font>\n",
    "\n",
    "The cdf ( `.cdf()` ) function gives us the probability that the variable will assume a value less than a specific one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I'm 183, what percentage of men are shorter than me?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.cdf(x=183, loc=173.9, scale=7.42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you're 183cm (6ft) or taller than you're taller than 89% of Portuguese men - score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Percent Point Function (PPF) </font> \n",
    "\n",
    "The PPF function `.ppf()` is the inverse of `.cdf()`; Instead of inputing a value and getting a probablity, we input a probability and get a quantile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below which height are 90% of men?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.ppf(q=0.9, loc=173.9, scale=7.42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 90% of men are shorter than 183.4 centimeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Probability Density Function </font>\n",
    "\n",
    "The PDF (`.pdf()`) function gives us the relatively likelihood of the variable assuming a certain value.\n",
    "\n",
    "For example, the relative likelihood of a randomly chosen man from this population being 170 cm tall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.pdf(x=170, loc=174, scale=8.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the relative likelihood of a randomly chosen man from this population being 150 cm tall:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.pdf(x=150, loc=174, scale=8.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see it's more likely than a Portuguese man is 170 cm than 150 cm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> 2. Binomial Distribution </font>\n",
    "    \n",
    "The binomial distribution is a discrete probability distribution that models the number of sucesses in a set of events whose output is binary (True/False, Success/Failure, this is also called a *dichotomic* variable).\n",
    "\n",
    "It describes random phenomenon such as the number of heads you'll get, when you flip a coin multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation is: \n",
    "\n",
    "## $$ B(n, p) $$\n",
    "\n",
    "The parameters are just the number of trials, $ n ∈ N_0 $, and the probability of success, $ p ∈ [0,1] $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Example </font>\n",
    "Let's model the number of heads we get when we flip a coin 10 times. This is a fair coin (unbiased), so the chance of getting heads at each trial is 50%.\n",
    "\n",
    "So our parameters are:\n",
    "\n",
    "> n = 10\n",
    "\n",
    "> p = 0.5 # we define the success event as getting heads\n",
    "\n",
    "\n",
    "So our binomial distribution is:\n",
    "## $ B(10, 0.5) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some 10000 points from this distribution. \n",
    "This means we'll performing 10000 experiments, in which we flip a coin 10 times.\n",
    "\n",
    "Since we are using a binomial distribution, we'll use `stats.binom`. \n",
    "\n",
    "We'll use its `.rsv` method to generated the data. You can see that here, `n` is the number of trials, and `p` is the probability of success in each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_data = stats.binom.rvs(size=10000, n=10, p=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(binomial_data, bins=50, kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, unlike the normal distribution, this is a discrete distribution, meaning that the random variable can only assume discrete integer values (the number of times the coin landed heads). It does however sort of look like a normal distribution, in the sense that it is symmetric, but that changes when you use a `p` different from 0.5.\n",
    "\n",
    "Let's now toss a biased coin. This a coin that is more likely to land on heads than tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_coin_data = stats.binom.rvs(size=10000, n=10, p=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(biased_coin_data, kde = False, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this biased coin is more likely to get more heads in 10 trials than the fair coin. The distribution \"shifted\" to the right, so to speak.\n",
    "\n",
    "Let's now use some functions to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Cumulative Distribution Function (CDF) </font>\n",
    "\n",
    "This function tells us the probability that the random variable will assume a value less (or equal) to the one you provide.\n",
    "\n",
    "Let's find out the probability of getting 7 heads or less in 10 trials, with a fair coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.binom.cdf(k=7, n=10,  p=0.5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to ask the question \"what is the probablity of getting at least 7 heads in 10 trails?\", you are actually asking \"What is the probability of NOT getting 6 or less heads in 10 trials\". \n",
    "\n",
    "This sounds a lot like calculating the **complementary** probability of the event `getting 6 or less heads`, which we would calculate by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see there is a ??% of getting 7 or more heads by flipping a fair coin 10 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Probability Mass Function (PMF) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we used `.pdf()` to check the probability density on a certain point of a continuous probability density function. However, the binomial distribution is a discrete probability distribution, so instead we use `.pmf()` to check the proportion of observations at a certain point.\n",
    "\n",
    "Let's find out the probability of getting **exactly** 3 heads in 6 trails, on our fair coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.binom.pmf(k=3, n=6, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'>  3. Geometric and Exponential Distributions </font>\n",
    "\n",
    "The geometric distribution is a discrete distribution, and it's useful for modelling things like number of times you need to flip a coin before you see heads.\n",
    "\n",
    "The geometric distribution is defined as \n",
    "\n",
    "## $$Geom(p)$$\n",
    "\n",
    "where p is the probability of success each trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 10000\n",
    "variable = stats.geom.rvs(p=0.5, size=n)\n",
    "sns.displot(variable, bins=100, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential distribution is the continuous analogue to the geometric distribution, and it's useful for modelling things like the time you need to wait before your train arrives, knowing that there is a train every 10 minutes. More specifically it models the time between events that happen at a constant *rate*.\n",
    "\n",
    "The exponential distribution is defined as\n",
    "\n",
    "## $$exp(\\lambda)$$\n",
    "\n",
    "where $\\lambda$ is the event rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 100000\n",
    "minutes_between_trains = 10\n",
    "#stats models uses the period between events instead of the rate of events per period\n",
    "variable = stats.expon.rvs(loc=minutes_between_trains, size=n)\n",
    "sns.displot(variable, kde = False, bins=100)\n",
    "sns.mpl.pyplot.title(\"Time between trains\", fontsize=20)\n",
    "sns.mpl.pyplot.xlabel(\"values (minutes)\", fontsize=15)\n",
    "sns.mpl.pyplot.ylabel(\"Frequencies\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> 4. Poisson Distribution </font>\n",
    "\n",
    "The **Poisson distribution** is useful when you want to model the probability of the number of times an event is likely to occur, within a certain timeframe.\n",
    "\n",
    "It's useful to model things such as the number of pacients an hospital will receive within an hour.\n",
    "\n",
    "While also a very useful distribution, we have a lot to cover, so I leave it up to you to learn more about it if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 100000\n",
    "minutes_between_trains = 10\n",
    "# in scipty the poisson works with event rate, so trains per hour\n",
    "trains_per_hour = 60 / minutes_between_trains\n",
    "variable = stats.poisson.rvs(mu=trains_per_hour,size=n)\n",
    "sns.displot(variable, kde= False, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#eb3483'> 5. Central Limit Theorem </font>\n",
    "\n",
    "Imagine you have an online clothing store. You are thinking of adding products to your store targetted towards millenials *(defined as people younger than 35 years old)*. **How can we figure out our customer average age?**\n",
    "\n",
    "Well, one thing we could do is ask every single one of our clients their age and we could calculate the **Population Mean**.\n",
    "\n",
    "However, that would be basically impossible, but what we can do is send a survey to a random group of our customers and calculate the **Sample Mean**. Then we can **estimate** the Population Mean using the Sample Mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define here the age of the actual population *(in real life we wouldn't have this value)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_ages1 = stats.norm.rvs(loc=18, scale=3, size=150000)\n",
    "population_ages2 = stats.norm.rvs(loc=45, scale=5, size=100000)\n",
    "population_ages = pd.Series(np.concatenate((population_ages1, population_ages2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_ages.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how the real distribution of the population looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(population_ages, bins=58);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality we would get a survey sample of our population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sample = population_ages.sample(500, random_state=0)\n",
    "age_sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, right? With just 500 people, we got a pretty good estimate of the mean population age.\n",
    "\n",
    "If we wanted to be even more sure, we could take several samples, to check if that value doesn't change widly a lot from sample to sample. You see where I'm getting at? We plot out sampling distribution!\n",
    "\n",
    "Let's do that then. We'll take a sample of 500, take the mean of that sample, and record that mean. We do that process and bunch of times.\n",
    "\n",
    "Let's try doing that 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_number = 0\n",
    "\n",
    "point_estimates = []        \n",
    "for x in range(50):          \n",
    "    sample = np.random.choice(a= population_ages, size=500)\n",
    "    point_estimates.append( sample.mean() )\n",
    "    seed_number += 1\n",
    "    \n",
    "pd.DataFrame(point_estimates).hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's starting to look like something even seen before, no? Let's take 1000 samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimates = []         \n",
    "for x in range(10000):         \n",
    "    sample = np.random.choice(a= population_ages, size=500)\n",
    "    point_estimates.append( sample.mean() )\n",
    "pd.DataFrame(point_estimates).hist(bins=30);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, isn't it? If we take the mean of a lot of samples, the resulting distribution will be a normal distribution! This is one of the most important concepts in probability theory, and it's called the **Central Limit Theorem**. It tells us that distribution of many sample means will be normally distributed, *regardless of the underlying variable distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem allows us to use statistical techniques that assume that our distribution is normal!\n",
    "\n",
    "Now, remember that we were trying to estimate the mean age of our population using the mean age of a sample. Let's assume we can only take a single sample of 500. \n",
    "\n",
    "We're going to get one single estimation for the mean population age, and because we only took one sample, that means our estimation is subject to randomness, and that randomness is described by a normal distribution.\n",
    "\n",
    "Maybe what we want is not to present a single value for our estimated population mean, but a confidence interval; An interval of values, for which we can say something like: \"We are 95% confident that the population mean is between X and Y.\"\n",
    "\n",
    "The good thing about our sampling distribution for the mean being a normal distribution, is that it makes it easy to calculate those confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> 6. Confidence Intervals </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we calculate a confidence interval is by taking the estimated mean and then adding and subtracting a margin of error.\n",
    "\n",
    "The formula for the margin of error is:\n",
    "\n",
    "# $ Z * \\frac{\\sigma}{\\sqrt{n}} $\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "__Z__ - it stands for Z-score, which is the number of standard deviations from the mean that you need to capture the desired confidence level. \n",
    "\n",
    ">For example, if we wanted a confidence interval of 95%, we could use a Z-score of 2, because roughly 95% of the data is within 2 standard deviations from the mean. But to be more exact, we can use `stats.norm.ppf()` to get the Z-score, by inputing the quantile.\n",
    "\n",
    "$ {\\sigma} $ - standard deviation of the population.\n",
    "\n",
    "> Uh oh. You might be asking yourself, how are we supposed to know the standard deviation of the population, if all we have access is a sample? We'll see ahead a stategy to deal with this.\n",
    "\n",
    "__n__ - the number of samples.\n",
    "\n",
    "Since we don't have access to the standard deviation of the population, we can use the standard deviation of the sample instead. But by doing this, we are introducing a source of error; To compensate for it, instead of using the Z-score, we'll use something called the T-score. \n",
    "\n",
    "The T-score comes from a special distribution, called the Student's T-distribution. It resembles the normal distribution, except it gets wider if the sample size is low, and as the sample size increases, it becomes equal to the Normal distribution.\n",
    "The T-distribution needs a parameter called the **degrees of freedom**, which is just the sample size minus 1.\n",
    "\n",
    "Let's see:\n",
    "\n",
    "![title](./media/t-dist.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as we increase our sample size, the T-distribution gets closer to the Normal distribution.\n",
    "\n",
    "So we end up with:\n",
    "\n",
    "# $ T * \\frac{\\sigma}{\\sqrt{n}} $\n",
    "\n",
    "In which T is the T-score, $\\sigma$ is the standard deviation of the sample, and n is the number of samples.\n",
    "\n",
    "Let's put it all together, and calculate the 95% confidence interval for the mean age of the population!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the mean age in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sample_age = age_sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the T-score. Since we want a 95% confidence interval, that means our significance level ($\\alpha$) is 0.05.\n",
    "\n",
    "And since the distribution has two tails, we need to do:\n",
    "\n",
    "1 - 0.95 = 0.05\n",
    "\n",
    "0.05 / 2 = 0.025\n",
    "\n",
    "1 - 0.025 = 0.975\n",
    "\n",
    "0.975 is then the quantile we want.\n",
    "\n",
    "`df`, aka degrees of freedom, is 499 because it's the sample size minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_critical = stats.t.ppf(q = 0.975, df=len(age_sample) - 1)\n",
    "t_critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see is very close to the `Z` value of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need $\\sigma$, which is the standard deviation for the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sample_age = age_sample.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $\\sqrt{n}$, which is just the square root for the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_of_n_samples = np.sqrt(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_margin = t_critical * std_sample_age / sqrt_of_n_samples\n",
    "\n",
    "confidence_interval = (mean_sample_age - error_margin,\n",
    "                       mean_sample_age + error_margin)\n",
    "\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, instead of calculating the confidence intervals by hand we can do it all using `t.interval()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.t.interval(alpha = 0.95,                               # Confidence level\n",
    "                 df= len(age_sample) - 1,                    # Degrees of freedom (sample - 1)\n",
    "                 loc = mean_sample_age,                      # Sample mean\n",
    "                 scale = std_sample_age / sqrt_of_n_samples) # Standard deviation of the sample\n",
    "                                                             # divided by the square root of the\n",
    "                                                             #number of samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
