# -*- coding: utf-8 -*-
"""OperationAPPLE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16xhvaTCoF8Ll1MFpsH_eN6TmhmL1Y44P

S&P 500 companies job titles classification model using Python's Natural Language Toolkit and Bayesian Multi-Class classifier
"""

# Importing packages and package features

import re
import pandas as pd
import nltk
nltk.download('stopwords') # <-----only need to run once every session
nltk.download('punkt')  # <-----only need to run once every session
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# Importing dataset
df = pd.read_excel(r"C:\Users\phuro\iXperience\Going Solo Internship\Phu - classification spreadsheet.xlsx")
df.dtypes

# Changing cell values in dataset

# Example:
      # df.loc[df['title'] == 'president,agriculture,turf,europe,cis,asia,africaeurity,regulatory,af']  
      # df.at[30200, 'title'] = 'president,agriculture,cis'

# Need discussion
      # df.loc[df['title'] == 'president,britain,united states,canada,ddbr,china,japan,korea'].index

# Cleaning, inspecting, and modifying dataset

drop_columns = df[df.columns.difference(['officertitle', '# of tickers', 'cumulative', 'CDF', 'title'])]
df.drop(columns=drop_columns, inplace=True)
df.head()

officertitles = df.officertitle
def longest(texts):
  curr_length = 0
  curr_text = ''
  for text in texts:
    if type(text) != str:
      continue
    elif len(text) > curr_length:
      curr_length = len(text)
      curr_text = text
  return curr_length, curr_text

print(longest(officertitles))
# longest(df.title)

# Create empty columns for component assignment when splitting title(s) by comma
df['COMP1'] = ''
df['COMP2'] = ''
df['COMP3'] = ''
df['COMP4'] = ''
df['COMP5'] = ''
df['COMP6'] = ''
df['COMP7'] = ''
df['COMP8'] = ''

# Which title in title col has the most components?
def max_comp_count(titles):
  max = 0
  most_comp_title = ''
  for title in titles:
    if type(title) == str:
      comp_count = title.split(',')
    if len(comp_count) > max:
      max = len(comp_count)
      most_comp_title = title
  return max, most_comp_title

print(max_comp_count(df.title))

df.head()

# Tokenize full officetitles and define features 

def get_title_features(title):
  features = {}
  word_tokens = nltk.word_tokenize(title)
  filtered_words = [word for word in word_tokens if word not in stop_words]

  for word in filtered_words:
    features['contains({})'.format(word.lower())] = True

  if len(filtered_words) > 0:
    first_key = 'first({})'.format(filtered_words[0].lower())
    last_key = 'last({})'.format(filtered_words[-1].lower())
    features[first_key] = True
    features[last_key] = True

  return features

test = get_title_features(df['officertitle'][0])
test

# Split training outcomes (title column) into COMPonents, one COMP per column

def splitter(titles):
  # for title in titles:
  for i in range(0, len(titles)):
    comp_num = 1
    title = titles.tolist()[i]
    if type(title) == str:
      components = title.split(',')
      for component in components:
        curr_column = 'COMP'+str(comp_num)
        df.at[i, curr_column] = component
        comp_num += 1
  return title

splitter(df.title)

df.head(31)

# Row inspection

df.loc[3505]

# Query my part (0 - 55490):

mp = df[0:55490]
mp.head()

# Check for wrong data types
for row in mp.itertuples(index=True):
  if type(row[1]) is not str:
    print(row[0])
    print('Got CHA')

# Classfier Training / build feature sets

# COMP1
comp1_features = [
    ( get_title_features(str(row[1])), row[6] )
    for row in mp.itertuples() if row[6] is not None
]

# COMP2
comp2_features = [
    ( get_title_features(str(row[1])), row[7] )
    for row in mp.itertuples() if row[7] is not None
]

# COMP3
comp3_features = [
    ( get_title_features(str(row[1])), row[8] )
    for row in mp.itertuples() if row[8] is not None
]

# COMP4
comp4_features = [
    ( get_title_features(str(row[1])), row[9] )
    for row in mp.itertuples() if row[9] is not None
]

# COMP5
comp5_features = [
    ( get_title_features(str(row[1])), row[10] )
    for row in mp.itertuples() if row[10] is not None
]

# COMP6
comp6_features = [
    ( get_title_features(str(row[1])), row[11] )
    for row in mp.itertuples() if row[11] is not None
]

# COMP7
comp7_features = [
    ( get_title_features(str(row[1])), row[12] )
    for row in mp.itertuples() if row[12] is not None
]

# COMP8
comp8_features = [
    ( get_title_features(str(row[1])), row[13] )
    for row in mp.itertuples() if row[13] is not None
]

len(comp8_features)

# Train and Test the classifier 

# COMP1
c1_size = int(len(comp1_features) * (1/2))
c1_train_set = comp1_features[c1_size:]
c1_test_set = comp1_features[:c1_size]
comp1_classifier = nltk.NaiveBayesClassifier.train(c1_train_set)
print("COMP1 classification accuracy: {}".format(
    nltk.classify.accuracy(comp1_classifier, c1_test_set)
))

# COMP2
c2_size = int(len(comp2_features) * (1/2))
c2_train_set = comp2_features[c2_size:]
c2_test_set = comp2_features[:c2_size]
comp2_classifier = nltk.NaiveBayesClassifier.train(c2_train_set)
print("COMP2 classification accuracy: {}".format(
    nltk.classify.accuracy(comp2_classifier, c2_test_set)
))

# COMP3
c3_size = int(len(comp3_features) * (1/2))
c3_train_set = comp3_features[c3_size:]
c3_test_set = comp3_features[:c3_size]
comp3_classifier = nltk.NaiveBayesClassifier.train(c3_train_set)
print("COMP3 classification accuracy: {}".format(
    nltk.classify.accuracy(comp3_classifier, c3_test_set)
))

# COMP4
c4_size = int(len(comp4_features) * (1/2))
c4_train_set = comp4_features[c4_size:]
c4_test_set = comp4_features[:c4_size]
comp4_classifier = nltk.NaiveBayesClassifier.train(c4_train_set)
print("COMP4 classification accuracy: {}".format(
    nltk.classify.accuracy(comp4_classifier, c4_test_set)
))

# COMP5
c5_size = int(len(comp5_features) * (1/2))
c5_train_set = comp5_features[c5_size:]
c5_test_set = comp5_features[:c5_size]
comp5_classifier = nltk.NaiveBayesClassifier.train(c5_train_set)
print("COMP5 classification accuracy: {}".format(
    nltk.classify.accuracy(comp5_classifier, c5_test_set)
))

# COMP6
c6_size = int(len(comp6_features) * (1/2))
c6_train_set = comp6_features[c6_size:]
c6_test_set = comp6_features[:c6_size]
comp6_classifier = nltk.NaiveBayesClassifier.train(c6_train_set)
print("COMP6 classification accuracy: {}".format(
    nltk.classify.accuracy(comp6_classifier, c6_test_set)
))

# COMP7
c7_size = int(len(comp7_features) * (1/2))
c7_train_set = comp7_features[c7_size:]
c7_test_set = comp7_features[:c7_size]
comp7_classifier = nltk.NaiveBayesClassifier.train(c7_train_set)
print("COMP7 classification accuracy: {}".format(
    nltk.classify.accuracy(comp7_classifier, c7_test_set)
))

# COMP8
c8_size = int(len(comp8_features) * (1/2))
c8_train_set = comp8_features[c8_size:]
c8_test_set = comp8_features[:c8_size]
comp8_classifier = nltk.NaiveBayesClassifier.train(c8_train_set)
print("COMP8 classification accuracy: {}".format(
    nltk.classify.accuracy(comp8_classifier, c8_test_set)
))







"""**Analysis Notes:**

* The following rows were dropped due to none or nonsensical values for officertitle: 

Format: row number (original value)

Row 1603 (NA), 1771 (-1), 3505, 3510, 5983, 8680, 13306 (see bekiw), 14496 (See Foonote (3)), 15091 (See below), 15756 (9212011 miami-dadeclerk.com), 17273 (** See below), 22618 (SEE FOOTNOTES 2  6), 23641 (PLEASE PROVIDE), 24537 (none), 25130 (see foot note), 25215 (See "Remarks" page), 26027 (-3), 28097, 28111 (See attached Exhibit 99.1.), 29787 (See attached Exhibit 99.1.), 29979 (94566), 30817 (See explanation), 33087 (See also footnote 4 5 and 7), 33272 (See Remakrs), 34187 (See footnote (6)), 34375, 35015 (See Footnote (8)), 36619 (See Foonote (2)), 36873 (See explanatory note.), 37457 (see notes), 38067 (For purposes of Section 16), 41590 (See attached Ex. 99.1), 43264 (need title), 44681 (SEE EXPLANATION BELOW(1)), 45674 (* See title below), 46258 (See "Explanation of Responses"), 46344 (See Exhibit 99.2), 46525, 48240 (Resigned Nov 30 2007)


"""

# Unused code archive

# index = df['title'].tolist().index(title)
# mp[mp['COMP8'] != '']
